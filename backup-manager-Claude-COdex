diff --git a/backup_manager.py b/backup_manager.py
new file mode 100644
index 0000000000000000000000000000000000000000..2c1df048af35ef0e8c89d9d761cd28cc82c26536
--- /dev/null
+++ b/backup_manager.py
@@ -0,0 +1,388 @@
+from __future__ import annotations
+
+import json
+import shutil
+import threading
+import time
+from datetime import datetime, timedelta
+from pathlib import Path
+from typing import Dict, List, Optional
+
+
+class BackupManager:
+    """Simple backup manager supporting hourly, daily, and model backups."""
+
+    def __init__(
+        self,
+        source_dirs: Optional[List[str]] = None,
+        backup_root: str = "backups",
+        enable_compression: bool = True,
+    ) -> None:
+        self.backup_root = Path(backup_root)
+        self.backup_root.mkdir(parents=True, exist_ok=True)
+
+        # Source directories to backup
+        self.source_dirs = source_dirs or ["state", "data", "models"]
+
+        # Backup directories
+        self.hourly_dir = self.backup_root / "hourly"
+        self.daily_dir = self.backup_root / "daily"
+        self.model_dir = self.backup_root / "models"
+
+        for directory in (self.hourly_dir, self.daily_dir, self.model_dir):
+            directory.mkdir(exist_ok=True)
+
+        self.enable_compression = enable_compression
+
+        self.auto_backup_enabled = False
+        self.hourly_thread: Optional[threading.Thread] = None
+        self.daily_thread: Optional[threading.Thread] = None
+
+        print("ðŸ’¾ Backup Manager initialized")
+        print(f"   Backup root: {backup_root}")
+        print(f"   Compression: {'Enabled' if enable_compression else 'Disabled'}")
+
+    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+    # BACKUP CREATION
+    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+
+    def create_hourly_backup(self) -> Optional[Path]:
+        """Create an hourly backup containing state files."""
+        timestamp = datetime.now().strftime("%Y%m%d_%H0000")
+        backup_name = f"hourly_{timestamp}"
+        backup_path = self.hourly_dir / backup_name
+
+        try:
+            backup_path.mkdir(exist_ok=True)
+
+            state_dir = Path("state")
+            if state_dir.exists():
+                for file in state_dir.glob("*.json"):
+                    dest = backup_path / file.name
+                    shutil.copy2(file, dest)
+
+            if self.enable_compression:
+                backup_path = self._compress_directory(backup_path)
+
+            print(f"âœ… Hourly backup created: {backup_name}")
+
+            self._cleanup_old_backups(self.hourly_dir, hours=24)
+
+            return backup_path
+
+        except Exception as exc:  # pragma: no cover - safety net
+            print(f"âŒ Hourly backup error: {exc}")
+            return None
+
+    def create_daily_backup(self) -> Optional[Path]:
+        """Create a daily backup containing the entire system state."""
+        timestamp = datetime.now().strftime("%Y%m%d")
+        backup_name = f"daily_{timestamp}"
+        backup_path = self.daily_dir / backup_name
+
+        try:
+            backup_path.mkdir(exist_ok=True)
+
+            for source_name in self.source_dirs:
+                source_dir = Path(source_name)
+
+                if not source_dir.exists():
+                    continue
+
+                dest_dir = backup_path / source_name
+                shutil.copytree(source_dir, dest_dir, dirs_exist_ok=True)
+
+            metadata = {
+                "backup_type": "daily",
+                "timestamp": datetime.now().isoformat(),
+                "source_dirs": self.source_dirs,
+                "compressed": self.enable_compression,
+            }
+
+            with open(backup_path / "metadata.json", "w", encoding="utf-8") as file:
+                json.dump(metadata, file, indent=2)
+
+            if self.enable_compression:
+                backup_path = self._compress_directory(backup_path)
+
+            print(f"âœ… Daily backup created: {backup_name}")
+
+            self._cleanup_old_backups(self.daily_dir, days=7)
+
+            return backup_path
+
+        except Exception as exc:  # pragma: no cover - safety net
+            print(f"âŒ Daily backup error: {exc}")
+            return None
+
+    def create_model_checkpoint(self, model_name: str = "rl_model") -> Optional[Path]:
+        """Create a model checkpoint backup."""
+        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+        backup_name = f"{model_name}_{timestamp}"
+        backup_path = self.model_dir / backup_name
+
+        try:
+            backup_path.mkdir(exist_ok=True)
+
+            model_source = Path("models")
+            if model_source.exists():
+                for file in model_source.glob(f"{model_name}.*"):
+                    dest = backup_path / file.name
+                    shutil.copy2(file, dest)
+
+            metadata = {
+                "model_name": model_name,
+                "timestamp": datetime.now().isoformat(),
+                "checkpoint_type": "weekly",
+            }
+
+            with open(backup_path / "metadata.json", "w", encoding="utf-8") as file:
+                json.dump(metadata, file, indent=2)
+
+            print(f"âœ… Model checkpoint created: {backup_name}")
+
+            self._cleanup_old_backups(self.model_dir, days=28)
+
+            return backup_path
+
+        except Exception as exc:  # pragma: no cover - safety net
+            print(f"âŒ Model checkpoint error: {exc}")
+            return None
+
+    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+    # RESTORE
+    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+
+    def restore_from_backup(self, backup_path: Path) -> bool:
+        """Restore sources from a backup archive or directory."""
+        try:
+            if not backup_path.exists():
+                print(f"âŒ Backup not found: {backup_path}")
+                return False
+
+            if backup_path.suffix == ".gz":
+                backup_path = self._decompress_directory(backup_path)
+
+            metadata_file = backup_path / "metadata.json"
+            if metadata_file.exists():
+                with open(metadata_file, encoding="utf-8") as file:
+                    metadata = json.load(file)
+                print(
+                    "ðŸ“‹ Backup info: "
+                    f"{metadata.get('backup_type')} - {metadata.get('timestamp')}"
+                )
+
+            for source_name in self.source_dirs:
+                source_backup = backup_path / source_name
+
+                if not source_backup.exists():
+                    continue
+
+                dest_dir = Path(source_name)
+                temp_backup: Optional[Path] = None
+
+                if dest_dir.exists():
+                    temp_backup = dest_dir.parent / f"{dest_dir.name}_temp_backup"
+                    if temp_backup.exists():
+                        shutil.rmtree(temp_backup)
+                    shutil.move(str(dest_dir), str(temp_backup))
+
+                shutil.copytree(source_backup, dest_dir, dirs_exist_ok=True)
+
+                if temp_backup and temp_backup.exists():
+                    shutil.rmtree(temp_backup)
+
+            print(f"âœ… Restored from backup: {backup_path.name}")
+            return True
+
+        except Exception as exc:  # pragma: no cover - safety net
+            print(f"âŒ Restore error: {exc}")
+            return False
+
+    def list_backups(self, backup_type: str = "all") -> List[Dict[str, object]]:
+        """Return a list of available backups."""
+        backups: List[Dict[str, object]] = []
+
+        dirs_to_check = []
+        if backup_type in ("hourly", "all"):
+            dirs_to_check.append(("hourly", self.hourly_dir))
+        if backup_type in ("daily", "all"):
+            dirs_to_check.append(("daily", self.daily_dir))
+        if backup_type in ("model", "all"):
+            dirs_to_check.append(("model", self.model_dir))
+
+        for backup_kind, directory in dirs_to_check:
+            for item in sorted(directory.iterdir(), reverse=True):
+                if item.is_dir() or item.suffix == ".gz":
+                    metadata_file = item / "metadata.json" if item.is_dir() else None
+
+                    backup_info: Dict[str, object] = {
+                        "type": backup_kind,
+                        "name": item.name,
+                        "path": str(item),
+                        "size_mb": self._get_size_mb(item),
+                        "created": datetime.fromtimestamp(item.stat().st_mtime).isoformat(),
+                    }
+
+                    if metadata_file and metadata_file.exists():
+                        with open(metadata_file, encoding="utf-8") as file:
+                            backup_info["metadata"] = json.load(file)
+
+                    backups.append(backup_info)
+
+        return backups
+
+    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+    # AUTO-BACKUP (Threading)
+    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+
+    def start_auto_backup(self) -> None:
+        """Start background threads for automatic backups."""
+        if self.auto_backup_enabled:
+            print("âš ï¸  Auto-backup already running")
+            return
+
+        self.auto_backup_enabled = True
+
+        self.hourly_thread = threading.Thread(target=self._hourly_backup_loop, daemon=True)
+        self.hourly_thread.start()
+
+        self.daily_thread = threading.Thread(target=self._daily_backup_loop, daemon=True)
+        self.daily_thread.start()
+
+        print("âœ… Auto-backup started")
+        print("   Hourly: Every hour")
+        print("   Daily: Every day at 00:00")
+
+    def stop_auto_backup(self) -> None:
+        """Stop automatic backup threads."""
+        self.auto_backup_enabled = False
+
+        if self.hourly_thread:
+            self.hourly_thread.join(timeout=2)
+        if self.daily_thread:
+            self.daily_thread.join(timeout=2)
+
+        print("â¸ï¸  Auto-backup stopped")
+
+    def _hourly_backup_loop(self) -> None:
+        while self.auto_backup_enabled:
+            now = datetime.now()
+            next_hour = now.replace(minute=0, second=0, microsecond=0) + timedelta(hours=1)
+            wait_seconds = (next_hour - now).total_seconds()
+
+            time.sleep(wait_seconds)
+
+            if self.auto_backup_enabled:
+                self.create_hourly_backup()
+
+    def _daily_backup_loop(self) -> None:
+        while self.auto_backup_enabled:
+            now = datetime.now()
+            next_day = (now + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
+            wait_seconds = (next_day - now).total_seconds()
+
+            time.sleep(wait_seconds)
+
+            if self.auto_backup_enabled:
+                self.create_daily_backup()
+
+    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+    # HELPER METHODS
+    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
+
+    def _compress_directory(self, directory: Path) -> Path:
+        archive = Path(shutil.make_archive(str(directory), "gztar", directory.parent, directory.name))
+
+        if directory.exists():
+            shutil.rmtree(directory)
+
+        return archive
+
+    def _decompress_directory(self, archive_path: Path) -> Path:
+        extract_path = archive_path.parent / archive_path.stem.replace(".tar", "")
+        shutil.unpack_archive(str(archive_path), str(extract_path))
+        return extract_path
+
+    def _cleanup_old_backups(
+        self,
+        backup_dir: Path,
+        days: Optional[int] = None,
+        hours: Optional[int] = None,
+    ) -> None:
+        if days is not None:
+            cutoff = datetime.now() - timedelta(days=days)
+        elif hours is not None:
+            cutoff = datetime.now() - timedelta(hours=hours)
+        else:
+            return
+
+        deleted_count = 0
+
+        for item in backup_dir.iterdir():
+            created = datetime.fromtimestamp(item.stat().st_mtime)
+
+            if created < cutoff:
+                if item.is_dir():
+                    shutil.rmtree(item)
+                else:
+                    item.unlink()
+
+                deleted_count += 1
+
+        if deleted_count > 0:
+            print(f"ðŸ—‘ï¸  Cleaned up {deleted_count} old backups from {backup_dir.name}")
+
+    def _get_size_mb(self, path: Path) -> float:
+        if path.is_file():
+            return path.stat().st_size / (1024 * 1024)
+
+        total = 0
+        for item in path.rglob("*"):
+            if item.is_file():
+                total += item.stat().st_size
+
+        return total / (1024 * 1024)
+
+    def get_storage_summary(self) -> Dict[str, Dict[str, float]]:
+        return {
+            "hourly": {
+                "count": len(list(self.hourly_dir.iterdir())),
+                "total_size_mb": self._get_size_mb(self.hourly_dir),
+            },
+            "daily": {
+                "count": len(list(self.daily_dir.iterdir())),
+                "total_size_mb": self._get_size_mb(self.daily_dir),
+            },
+            "model": {
+                "count": len(list(self.model_dir.iterdir())),
+                "total_size_mb": self._get_size_mb(self.model_dir),
+            },
+        }
+
+
+if __name__ == "__main__":
+    manager = BackupManager(
+        source_dirs=["state", "data", "models"],
+        backup_root="backups",
+        enable_compression=True,
+    )
+
+    print("\nðŸ“¦ Creating manual backups...")
+    manager.create_hourly_backup()
+    manager.create_daily_backup()
+    manager.create_model_checkpoint("rl_model")
+
+    print("\nðŸ“‹ Listing backups...")
+    backups = manager.list_backups("all")
+    for backup in backups[:5]:
+        print(f"  {backup['type']}: {backup['name']} ({backup['size_mb']:.1f} MB)")
+
+    print("\nðŸ’¾ Storage Summary:")
+    summary = manager.get_storage_summary()
+    for backup_type, info in summary.items():
+        print(f"  {backup_type}: {info['count']} backups ({info['total_size_mb']:.1f} MB)")
+
+    # manager.start_auto_backup()
+    # time.sleep(3600)
+    # manager.stop_auto_backup()
